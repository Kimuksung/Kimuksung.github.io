---
layout: post
title:  "RDS Data Dev환경으로 옮기기"
author: Kimuksung
categories: [ RDS ]
tags: [ RDS, EC2, Mysql ]
# 댓글 기능
comments: False
image: "https://i.ibb.co/wYVBLFY/rds.png"
---

Production Data와 Dev Data의 Sync를 종종 맞추어 개발하고 싶다고 하여 개발한 과정입니다.

AWS MDS 등 여러 자료들을 찾아보았으나 Mysql command를 이용하여 데이터를 옮겼습니다.

현재 데이터 구조는 EC2 → RDS구조로 되어있어 일반적인 접속이 아닌 EC2 Instance 접속 후에 실행하여야 합니다.

##### mysql 설치
---
```bash
# MAC
$ brew install mysql
# Ubuntu
$ sudo apt-get install mysql
```

##### mysql instance 접속
---
- RDS Endpoint
- Port
- User

```python
$ mysql -h <RDS 엔드포인트 주소> -P <포트 번호> -u <사용자 이름> -p
```

##### mysql database dump
---
- mysqldump [옵션] db_name [table_name] > backup_filename
- mysqldump [옵션] --databases [옵션] db_name1, db_name2, .... > backup_filename
- mysqldump [옵션] --all-databases [옵션] > backup_filename
    
    ```
    [옵션] 
     -A, --all-databases     : 모든 DB를 덤프
    
     --add-locks                : 덤프 전에 lock 덤프 후에 unlock
    
     --add-drop-table         : 덤프 이후에 출력물의 앞에 drop table 명령 추가 (복구 위해)
    
     -B, --databases          : 여러 DB를 동시에 덤프할 때 사용
    
     -f, --force                   : 에러를 무시
    
     -h, --host                   : 지정한 호스트의 데이터를 덤프
    
     -t                  : data만 덤프
    
     -d                 : 데이터를 제외하고 스키마만 덤프
    
     -p                 : 사용자의 암호를 지정
    
     -P                 : 포트번호 지정
    
     -u                 : 사용자명 지정
    
    ```
    
    ```python
    $ mysqldump -h <RDS 엔드포인트 주소> -P <포트 번호> -u <사용자 이름> -p <데이터베이스 이름> > dump.sql
    ```
    

error : `Warning: A partial dump from a server that has GTIDs will by default include the GTIDs of all transactions, even those that changed suppressed parts of the database. If you don't want to restore GTIDs, pass --set-gtid-purged=OFF. To make a complete dump, pass --all-databases --triggers --routines --events.
Warning: A dump from a server that has GTIDs enabled will by default include the GTIDs of all transactions, even those that were executed during its extraction and might not be represented in the dumped data. This might result in an inconsistent data dump.
In order to ensure a consistent backup of the database, pass --single-transaction or --lock-all-tables or --master-data.
mysqldump: Couldn't execute 'SELECT COLUMN_NAME,                       JSON_EXTRACT(HISTOGRAM, '$."number-of-buckets-specified"')                FROM information_schema.COLUMN_STATISTICS                WHERE SCHEMA_NAME = 'baro-dev-tell' AND TABLE_NAME = 'admin';': Unknown table 'COLUMN_STATISTICS' in information_schema (1109)`

- 해결 방안 = **`--set-gtid-purged=OFF --column-statistics=0`**
- mysqldump 8에서 기본적으로 활성화 된 새로운 플래그 때문 - [참조](https://qastack.kr/server/912162/mysqldump-throws-unknown-table-column-statistics-in-information-schema-1109)
    
    ```python
    # 해결 방안
    $ mysqldump --set-gtid-purged=OFF --column-statistics=0 -h <RDS 엔드포인트 주소> -P <포트 번호> -u <사용자 이름> -p <데이터베이스 이름> > dump.sql
    ```
    

##### Dump Data Upload
---
- writer Instance mysql 접속 → Database 설정 → 실행
- 100만 여건을 돌렸을 때에도 10초안에 종료
- 데이터 베이스 접근 후 **`source`** 명령어로 upload 실행

```bash
$ mysql -h <RDS 엔드포인트 주소> -P <포트 번호> -u <사용자 이름> -p
```

```bash
$ CREATE DATABASE `database`;
# read-instance 접속 시 Error
ERROR 1290 (HY000): The MySQL server is running with the --read-only option so it cannot execute this statement
```

```bash
# select databases
$ use database;
# upload
$ source ./dump.sql
```

##### 100만개 만들어서 테스트 하기
---
- Python+Join 으로 100만여개 테이블 구성
- Mysql → dump.sql = 5초 이내로 종료
- dump.sql → Mysql = 쿼리를 일일히 입력 ( 초당 25000개 정도 처리)
- 한글 깨지지 않는 부분 확인

```python
$ pip install mysql-connector-python
```

```python
import mysql.connector

def query_executor(cursor):
    sql = "select * from dumptest;"
    cursor.execute(sql)

# ------------------------------------------------------#
if __name__ == "__main__":
    try:
        mysql_con = mysql.connector.connect(
            host="rds-endpoint",
            port=3306,
            user="user",
            password="pw",
            database="database"
        )

        mysql_cursor = mysql_con.cursor(dictionary=True)

        query_executor(mysql_cursor)

        for row in mysql_cursor:
            print(row)

        mysql_cursor.close()

    except Exception as e:
        print(e.message)

    finally:
        if mysql_con is not None:
            mysql_con.close()
```

```python
import random
import string
import mysql.connector

# MySQL 연결 설정
connection = mysql.connector.connect(
  host="rds-endpoint",
  port=3306,
  user="user",
  password="pw",
  database="database"
)

# 테이블 생성 쿼리
create_table_query = """
CREATE TABLE dumptest (
  id INT AUTO_INCREMENT PRIMARY KEY,
  column1 VARCHAR(255),
  column2 INT
)
"""

# 데이터 삽입 쿼리
insert_query = """
INSERT INTO dumptest (column1, column2)
VALUES (%s, %s)
"""

# 데이터 개수 설정
num_data = 1000

# 데이터 삽입을 위한 루프
for i in range(num_data):
  # 임의의 데이터 생성
  data1 = ''.join(random.choices(string.ascii_letters, k=10))
  data2 = random.randint(1, 100)
  # 데이터 삽입
  cursor = connection.cursor()
  cursor.execute(insert_query, (data1, data2))
  cursor.close()

# 변경 사항 커밋
connection.commit()

# 연결 종료
connection.close()
```

```sql
create table dumptest2 as
select a.*, b.column1 as b_col1, b.column2 as b_col2
from dumptest as a
cross join dumptest as b
```

##### 용량 및 CPU 지표 확인

---

- dump.sql = 39M
- 실제 DB 크기 = 168M
- Production Mysql = 480M
- Write Database(주황색 선) 의 이용량이 12.1 → 16.2로 4%정도 늘어난것을 알 수 있다. ( 저점 대비 )

```python
SELECT SUM(data_length+index_length)/1024/1024 used_MB, SUM(data_free)/1024/1024 free_MB 
FROM information_schema.tables
```

```python
$ du -sh dump.sql
```

![](https://i.ibb.co/nLq4xFq/2023-06-15-4-44-10.png)

##### 참조
---
- [https://velog.io/@k904808/AWS-RDS-hotbackupmysqldump안-될-때](https://velog.io/@k904808/AWS-RDS-hotbackupmysqldump%EC%95%88-%EB%90%A0-%EB%95%8C)
- https://semoa.tistory.com/172
- https://qastack.kr/server/912162/mysqldump-throws-unknown-table-column-statistics-in-information-schema-1109